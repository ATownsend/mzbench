{
    "docs": [
        {
            "location": "/", 
            "text": "Welcome to MZBench\n\n\nExpressive, scalable load testing tool\n\n\n\n\n\n\nMZBench\n helps software testers and developers test their products under huge load. By testing your product with MZBench before going to production, you reduce the risk of outages under real life highload. \n\n\nMZBench runs test scenarios on many machines simultaneously, maintaining millions of connections, which make it suitable even for large scale products.\n\n\nMZBench is:\n\n\n\n\nCloud-aware:\n MZBench allocates nodes directly from Amazon EC2. \n\n\nScalable:\n tested with 100 nodes and millions of connections.\n\n\nExtendable:\n write your own \ncloud plugins\n and \nworkers\n. \n\n\nOpen-source:\n MZBench is released under the \nBSD license\n.\n\n\n\n\nInstallation\n\n\nTo use MZBench, you\nll need:\n\n\n\n\nErlang R17\n\n\nC++ compiler\n\n\nPython 2.6 or 2.7 with pip\n\n\n\n\nMost UNIX systems have C++ compiler and Python preinstalled.\n\n\nErlang is available in the \nofficial repositories on most GNU/Linux distros\n. If your distro doesn\nt have Erlang R17, \nbuild it from source\n.  \n\n\nDownload MZBench from GitHub and install Python requirements:\n\n\n$ git clone https://github.com/machinezone/mzbench\n$ sudo pip install -r mzbench/requirements.txt \n\n\n\n\nQuickstart\n\n\nStart the MZBench server on localhost:\n\n\n$ cd mzbench\n$ ./bin/mzbench start_server\nExecuting make -C /path/to//mzbench/bin/../server generate\nExecuting /path/to//mzbench/bin/../server/_build/default/rel/mzbench_api/bin/mzbench_api start\n\n\n\n\n\n\nNote\n\n\nThe first server start takes a few minutes. The shell will not respond, which is OK; please be patient. Further starts will be much faster.\n\n\n\n\nWhen the server is running, launch an example benchmark:\n\n\n$ ./bin/mzbench run examples/ramp.erl\n{\n    \nstatus\n: \npending\n, \n    \nid\n: 6\n}\nstatus: running                       00:09\n\n\n\n\nGo to \nlocalhost:4800\n and see the benchmark live status:\n\n\n\n\nHow It Works\n\n\nMZBench runs your test scenarios on many \nnodes\n, simultaneously. This allows it to put extraordinarily high load on the target system\u2014we\nre talking about \nmillions\n of simultaneous connections here.\n\n\nNode\n is a machine, virtual or physical, that runs your scenarios. In real-life testing, MZBench is used with a cloud service like Amazon EC2 that provides nodes on demand. Alternatively, you can manually list the available node hosts. Anyway, you have to provide MZBench the machines to run on. If there\ns not enough nodes to run all the jobs at the same time, MZBench evenly distributes the jobs between the available nodes.\n\n\nThere\ns one node that doesn\nt run scenarios\u2014the \ndirector node\n. It collects the metrics from the other nodes and runs \npost and pre hooks\n. So, if you want to run jobs on 10 nodes, reserve 11.\n\n\n\n\nWhen the MZBench server runs your scenarios, it allocates the nodes, prepares them, and distributes the jobs. During the test run, the nodes send the collected data to the director node which then submits them to the server. The server uses the data to render graphs and show stats:\n\n\n\n\nTo know what kind of jobs MZBench can run, it\ns important to understand the concept of a \nworker\n.\n\n\nWorker\n is an Erlang module that provides functions for test scenarios. A worker may implement a common protocol like HTTP or XMPP, or a specific routine that is relevant only for a particular test case. It also implements the related metrics.\n\n\nMZBench ships with workers for \nHTTP\n and \nXMPP\n protocols and a worker that \nexecutes shell commands\n. This should be enough for most common test cases, but you can use your own workers in necessary.\n\n\nRead Next\n\n\n\n\nHow to write scenarios \u2192\n\n\nHow to control MZBench from command line \u2192\n\n\nHow to deploy MZBench \u2192\n\n\nHow to write your own worker \u2192", 
            "title": "Overview"
        }, 
        {
            "location": "/#welcome-to-mzbench", 
            "text": "Expressive, scalable load testing tool    MZBench  helps software testers and developers test their products under huge load. By testing your product with MZBench before going to production, you reduce the risk of outages under real life highload.   MZBench runs test scenarios on many machines simultaneously, maintaining millions of connections, which make it suitable even for large scale products.  MZBench is:   Cloud-aware:  MZBench allocates nodes directly from Amazon EC2.   Scalable:  tested with 100 nodes and millions of connections.  Extendable:  write your own  cloud plugins  and  workers .   Open-source:  MZBench is released under the  BSD license .", 
            "title": "Welcome to MZBench"
        }, 
        {
            "location": "/#installation", 
            "text": "To use MZBench, you ll need:   Erlang R17  C++ compiler  Python 2.6 or 2.7 with pip   Most UNIX systems have C++ compiler and Python preinstalled.  Erlang is available in the  official repositories on most GNU/Linux distros . If your distro doesn t have Erlang R17,  build it from source .    Download MZBench from GitHub and install Python requirements:  $ git clone https://github.com/machinezone/mzbench\n$ sudo pip install -r mzbench/requirements.txt", 
            "title": "Installation"
        }, 
        {
            "location": "/#quickstart", 
            "text": "Start the MZBench server on localhost:  $ cd mzbench\n$ ./bin/mzbench start_server\nExecuting make -C /path/to//mzbench/bin/../server generate\nExecuting /path/to//mzbench/bin/../server/_build/default/rel/mzbench_api/bin/mzbench_api start   Note  The first server start takes a few minutes. The shell will not respond, which is OK; please be patient. Further starts will be much faster.   When the server is running, launch an example benchmark:  $ ./bin/mzbench run examples/ramp.erl\n{\n     status :  pending , \n     id : 6\n}\nstatus: running                       00:09  Go to  localhost:4800  and see the benchmark live status:", 
            "title": "Quickstart"
        }, 
        {
            "location": "/#how-it-works", 
            "text": "MZBench runs your test scenarios on many  nodes , simultaneously. This allows it to put extraordinarily high load on the target system\u2014we re talking about  millions  of simultaneous connections here.  Node  is a machine, virtual or physical, that runs your scenarios. In real-life testing, MZBench is used with a cloud service like Amazon EC2 that provides nodes on demand. Alternatively, you can manually list the available node hosts. Anyway, you have to provide MZBench the machines to run on. If there s not enough nodes to run all the jobs at the same time, MZBench evenly distributes the jobs between the available nodes.  There s one node that doesn t run scenarios\u2014the  director node . It collects the metrics from the other nodes and runs  post and pre hooks . So, if you want to run jobs on 10 nodes, reserve 11.   When the MZBench server runs your scenarios, it allocates the nodes, prepares them, and distributes the jobs. During the test run, the nodes send the collected data to the director node which then submits them to the server. The server uses the data to render graphs and show stats:   To know what kind of jobs MZBench can run, it s important to understand the concept of a  worker .  Worker  is an Erlang module that provides functions for test scenarios. A worker may implement a common protocol like HTTP or XMPP, or a specific routine that is relevant only for a particular test case. It also implements the related metrics.  MZBench ships with workers for  HTTP  and  XMPP  protocols and a worker that  executes shell commands . This should be enough for most common test cases, but you can use your own workers in necessary.", 
            "title": "How It Works"
        }, 
        {
            "location": "/#read-next", 
            "text": "How to write scenarios \u2192  How to control MZBench from command line \u2192  How to deploy MZBench \u2192  How to write your own worker \u2192", 
            "title": "Read Next"
        }, 
        {
            "location": "/scenarios/", 
            "text": "Scenarios\n describe the behavior you want MZBench to emulate during the benchmark. If you\nre testing an online store, your scenario will probably include opening a product page and adding the product to cart. For a search service, the scenario may be searching for a random word. You get the idea.\n\n\nIn MZBench, scenarios are .erl files written in a simple \nDSL\n. It looks a lot like Erlang but is much simpler.\n\n\nRead on to learn how to write test scenarios for MZBench.\n\n\nMZBench test scenarios consist of \nlists\n, \ntuples\n, and \natoms\n:\n\n\n\n\nA \nlist\n is a comma-separated sequence enclosed in square brackets: \n[A, B, C]\n\n\nA \ntuple\n is a comma-separated sequence enclosed in curly braces: \n{A, B, C}\n\n\nAtoms\n are reserved words\u2014function names, rate units, conditions. Basically, anything that\ns not a list, a tuple, a number, or a string is an atom: \nprint, make_install, lte, rpm\n  \n\n\n\n\nThe whole scenario is a list of tuples with a dot at the end:\n\n\n[\n    {function1, param1, param2},\n    {function2, [{param_name, param_value}]},\n    ...\n].\n\n\n\n\nEach of these tuples is called a \nstatement\n. A statement represents a function call: the first item is the function name, the others are the params to pass to it. Each param in its turn can also be a list, a tuple, or an atom.\n\n\nSome statements only appear at the top level of a scenario. They\nre called \ntop-level statements\n. There\nre two kinds of top-level statements: \ndirectives\n and \npools\n.\n\n\nSee live examples of MZBench scenarios on GitHub \u2192\n\n\nDirectives\n\n\nDirectives\n prepare the system for the benchmark and clean it up after it. This includes installing an external \nworker\n on test nodes, registering resource files, checking conditions, and executing shell commands before and after the test.\n\n\nmake_install\n\n\n{make_install, [{git, \nURL\n}, {branch, \nBranch\n}, {dir, \nDir\n}]}`\n\n\n\n\nInstall an external worker from a remote git repository on the test nodes before running the benchmark.\n\n\nMZBench downloads the worker and builds a .tgz archive, which is then distributed among the nodes and used in future provisions.\n\n\nThe following actions are executed during \nmake_install\n:\n\n\n$ git clone \nURL\n temp_dir\n$ cd temp_dir\n$ git checkout \nBranch\n\n$ cd \nDir\n\n$ make generate_tgz\n\n\n\n\nIf \nbranch\n is not specified, the default git branch is used.\n\n\nIf \ndir\n is not specified, \n.\n is used.\n\n\ninclude_resource\n\n\n{include_resource, \nResourceName\n, \nFileName\n, \nType\n}\n{include_resource, \nResourceName\n, \nFileURL\n, \nType\n}`\n\n\n\n\nRegister a \nresource file\n as \nResourceName\n.\n\n\nIf the file is on your local machine, put it in the same directory where you invoke \nmzbench run\n.\n\n\nType\n is one of the following atoms:\n\n\n\n\ntext\n\n\nPlain text file, interpreted as a single string.\n\n\njson\n\n\nJSON file. Lists are interpreted as \nErlang lists\n, objects are interpreted as \nErlang maps\n.\n\n\ntsv\n\n\nFile with tabulation separated values, interpreted as a list of lists.\n\n\nerlang\n\n\nErlang source file, interpreted directly as an \nErlang term\n.\n\n\nbinary\n\n\nCustom binary (image, executable, archive, etc.), not interpreted.\n\n\n\n\npre_hook and post_hook\n\n\n{pre_hook, \nActions\n}\n{post_hook, \nActions\n}\n\n\n\n\nRun actions before and after the benchmark. Two kinds of actions are supported: \nexec commands\n and \nworker calls\n:\n\n\nActions = [Action]\nAction = {exec, Target, BashCommand}\n    | {worker_call, WorkerMethod, WorkerModule}\n    | {worker_call, WorkerMethod, WorkerModule, WorkerType}\nTarget = all | director\n\n\n\nExec commands\n let you to run any shell command on all nodes or only on the director node.\n\n\nWorker calls\n are functions defined by the worker. They can be executed only on the director node. Worker calls are used to update the \nenvironment variables\n used in the benchmark.\n\n\nassert\n\n\n{assert, always, \nCondition\n}\n{assert, \nTime\n, \nCondition\n}\n\n\n\n\nCheck if the condition \nCondition\n is satisfied throughout the entire benchmark or at least for the amount of time \nTime\n.\n\n\n\n\nTime\n is a tuple \n{\nDuration\n, (ms|sec|min|h)}\n, e.g. \n{1, sec}\n.\n\n\nCondition\n is a comparison of two value and is defined as a tuple \n{\nOperation\n, \nOperand1\n, \nOperand2\n}\n.\n\n\nOperation\n is one of four atoms:\n\n\n\n\nlt\n\n\nLess than.\n\n\ngt\n\n\nGreater than.\n\n\nlte\n\n\nLess than or equal to.\n\n\ngte\n\n\nGreater than or equal to.\n\n\n\n\nOperand1\n and \nOperand2\n are the values to compare. They can be integers, floats, or \nmetrics\n values.\n\n\nMetrics\n are numerical values collected by the worker during the benchmark. To get the metric value, put its name between double quotation marks:\n\n\n{gt, \nhttp_ok\n, 20}\n\n\n\n\nThe \nhttp_ok\n metric is provided by the \nsimple_http\n worker. This condition passes if the number of successful HTTP responses is greater than 20.    \n\n\nPools\n\n\nPool\n represents a sequence of \njobs\n\u2014statements to run. The statements are defined by the \nworker\n and \nMZBench\ns standard library\n. The jobs are evenly distributed between nodes, so they can be executed in parallel.\n\n\nHere\ns a pool that sends HTTP GET requests to two sites on 10 nodes in parallel:\n\n\n    [ {pool,\n        [ {size, 10}, {worker_type, simple_http} ],\n        [\n            {get, \nhttp://example.com\n},\n            {get, \nhttp://foobar.com\n} \n        ]\n    } ].\n\n\n\n\nThe \nget\n statement is provided by the built-in \nsimple_http\n worker.\n\n\nThe first param in the \npool\n statement is a list of \npool options\n.\n\n\nPool Options\n\n\nsize\n\n\nrequired\n\n\n{size, \nNumberOfJobs\n}`\n\n\n\n\nHow many times you want the pool executed.\n\n\nIf there\ns enough nodes and \nworker_start\n is not set, MZBench will start the jobs simultaneously and run them in parallel.\n\n\nNumberOfJobs\n is any positive number.\n\n\nworker_type\n\n\nrequired\n\n\n{worker_type, \nWorkerName\n}\n\n\n\n\nThe worker that provides statements for the jobs.\n\n\n\n\nHint\n\n\nA pool uses exactly one worker. If you need multiple workers in the benchmark, just write a pool for each one.\n\n\n\n\nworker_start\n\n\n{worker_start, {linear, \nRate\n}}\n{worker_start, {poisson, \nRate\n}}\n{worker_start, {exp, \nScale\n, \nTime\n}}\n{worker_start, {pow, \nExponent\n, \nScale\n, \nTime\n}}\n\n\n\n\nStart the jobs with a given rate:\n\n\n\n\nlinear\n\n\nConstant rate \nRate\n, e.g. 10 per minute.\n\n\n\n\n\n\nRate\n is a tuple \n{\nN\n, (rps|rpm|rph)}\n, e.g. \n{10, rps}\n. It means \nN\n jobs per second, minute, or hour.\n\n\n\n\npoisson\n\n\nRate defined by a \nPoisson process\n with \u03bb = \nRate\n.\n\n\nexp\n\n\n\n\nStart jobs with \nexponentially growing\n rate with the scale factor \nScale\n:\n\n\nScale \u00d7 e\nTime\n\n\n\n\npow\n\n\n\n\nStart jobs with rate growing as a \npower function\n with the exponent \nExponent\n and the scale factor \nScale\n:\n\n\nScale \u00d7 Time\nExponent\n\n\n\n\n\n\nYou can customize and combine rates:\n\n\nthink_time\n\n\n{think_time, \nTime\n, \nRate\n}\n\n\n\n\nStart jobs with rate \nRate\n for a second, then sleep for \nTime\n and repeat.\n\n\nramp\n\n\n{ramp, linear, \nStartRate\n, \nEndRate\n}\n\n\n\n\nLinearly change the rate from \nStartRate\n at the beginning of the pool to \nEndRate\n at its end.\n\n\ncomb\n\n\n{comb, \nRate1\n, \nTime1\n, \nRate2\n, \nTime2\n, ...}\n\n\n\n\nStart jobs with rate \nRate1\n for \nTime1\n, then switch to \nRate2\n for \nTime2\n, etc.\n\n\nLoops\n\n\nLoop\n is a sequence of statements executed over and over for a given time.\n\n\nA loop looks similar to a \npool\n\u2014it consists of a list of \noptions\n and a list statements to run:\n\n\n{loop, [\n        {time, \nTime\n},\n        {rate, \nRate\n},\n        {parallel, \nN\n},\n        {iterator, \nName\n},\n        {spawn, \nSpawn\n}\n    ],\n    [\n        \nStatement1\n,\n        \nStatement2\n,\n        ...\n    ]\n}\n\n\n\n\nHere\ns a loop that sends HTTP GET requests for 30 seconds with a growing rate of 1 \u2192 5 rps:\n\n\n{loop, [\n        {time, {30, sec}},\n        {rate, {ramp, linear, {1, rps}, {5, rps}}}\n    ],\n    [\n        {get, \nhttp://example.com\n}\n    ]\n}\n\n\n\n\nYou can put loops inside loops. Here\ns a nested loop that sends HTTP GET requests for 30 seconds, increasing the rate by 1 rps every three seconds:\n\n\n{loop, [\n        {time, {30, sec}},\n        {rate, {10, rpm}},\n        {iterator, \ni\n}\n    ],\n    [\n        {loop, [\n                {time, {3, sec}}, \n                {rate, {{var, \ni\n}, rps}}\n            ],\n            [\n                {get, \nhttp://google.com\n}\n            ]\n        }\n    ]\n}\n\n\n\n\nThe difference between these two examples is that in the first case the rate is growing smoothly and in the second one it\ns growing in steps.\n\n\nLoop options\n\n\ntime\n\n\nrequired\n\n\n{time, \nTime\n}\n\n\n\n\nRun the loop for \nTime\n.\n\n\nrate\n\n\n{rate, \nRate\n}\n\n\n\n\nRepeat the loop with the \nRate\n rate.\n\n\nparallel\n\n\n{parallel, \nN\n}\n\n\n\n\nRun \nN\n iterations of the loop in parallel.\n\n\niterator\n\n\n{iterator, \nIterName\n}\n\n\n\n\nDefine a variable named \nIterName\n inside the loop that contains the current iteration number. It can be accessed with \n{var, \nIterName\n}\n.\n\n\nspawn\n\n\n{spawn, (true|false)}\n\n\n\n\nIf \ntrue\n, every iteration runs in a separate, spawned process. Default is \nfalse\n.\n\n\nResource Files\n\n\nResource file\n is an external data source for the benchmark.\n\n\nTo declare a resource file for the benchmark, use \ninclude_resource\n.\n\n\nOnce the resource file is registered, its content can be included at any place in the scenario using the \nresource\n statement: \n{resource, \nResourceName\n}\n.\n\n\nFor example, suppose we have a file \nnames.json\n:\n\n\n[\n    \"Bob\",\n    \"Alice\",\n    \"Guido\"\n]\n\n\n\nHere\ns how you can use this file in a scenario:\n\n\n[\n    {include_resource, names, \nnames.json\n, json},\n    {pool, [\n            {size, 3},\n            {worker_type, dummy_worker}\n        ],\n        [\n            {loop, [\n                    {time, {5, sec}},\n                    {rate, {1, rps}}\n                ],\n                [\n                    {print, {choose, {resource, names}}} % print a random name from the file\n                ]\n            }\n        ]\n    }\n].\n\n\n\n\nStandard Library\n\n\nEnvironment Variables\n\n\nEnvironment variables\n are global values that can be accessed at any point of the benchmark. They are useful to store the benchmark global state like its total duration, or global params like the execution speed.\n\n\nTo set an environment variable, call \nmzbench\n with the \n--env\n param:\n\n\n$ ./bin/mzbench run --env foo=bar --env n=42\n\n\n\n\nvar\n\n\n{var, \nVarName\n}\n{var, \nVarName\n, \nDefaultValue\n}\n\n\n\n\nTo get the value of a variable, refer to it by the name: \n{var, \"\nVarName\n\"}\n.\n\n\n{var, \nfoo\n} % returns \nbar\n\n{var, \nn\n} % returns \n42\n, a string\n\n\n\n\nIf you refer to an undefined variable, the benchmark crashes. You can avoid this by setting a default value for the variable: \n{var, \"\nVarName\n\", \nDefaultValue\n}\n:\n\n\n{var, \nanothervar\n, \nFoo\n} % returns \nFoo\n if anothervar is not set\n\n\n\n\nnumvar\n\n\n{numvar, \nVarName\n}\n{numvar, \nVarName\n, \nDefaultValue\n}\n\n\n\n\nBy default, variable values are considered strings. To get a numerical value (integer or float), use \n{numvar, \"VarName\"}\n:\n\n\n{numvar, \nn\n} % returns 42, an integer.\n\n\n\n\nParallelization and Syncing\n\n\nparallel\n\n\n{parallel, \nStatement1\n, \nStatement2\n, ...}\n\n\n\n\nExecute multiple statements in parallel. Unlike executing statements in a pool, this way all statements are executed on the same node.\n\n\nset_signal\n\n\n{set_signal, \nSignalName\n}\n{set_signal, \nSignalName\n, \nCount\n]}\n\n\n\n\nEmit a global signal \nSignalName\n.\n\n\nIf \nCount\n is specified, the signal is emitted \nCount\n times.\n\n\nSignalName\n is a string, atom, number, or, in fact, any \nErlang term\n.\n\n\nwait_signal\n\n\n{wait_signal, \nSignalName\n}\n{wait_signal, \nSignalName\n, \nCount\n}\n\n\n\n\nWait for the global signal \nSignalName\n to be emitted. If \nCount\n is specified, wait for the signal to be emitted \nCount\n times.\n\n\nErrors Handling\n\n\nignore_failure\n\n\n{ignore_failure, \nStatement\n}\n\n\n\n\nExecute the statement \nStatement\n and continue with the benchmark even if it fails.\n\n\nIf the statement succeeds, its result is returned; otherwise, the failure reason is returned.\n\n\nRandomization\n\n\nrandom_number\n\n\n{random_number, \nMin\n, \nMax\n}\n{random_number, \nMax\n}\n\n\n\n\nReturn a random number between \nMin\n and \nMax\n, including \nMin\n and not including \nMax\n.\n\n\n{random_number, \nMax\n}\n is equivalent to \n{random_number, 0, \nMax\n}\n\n\nrandom_list\n\n\n{random_list, \nSize\n}\n\n\n\n\nReturn a list of random integer of length \nSize\n.\n\n\nrandom_binary\n\n\n{random_binary, \nSize\n}\n\n\n\n\nReturn a binary sequence of \nSize\n random bytes.\n\n\nchoose\n\n\n{choose, \nN\n, \nList\n}\n{choose, \nList\n}\n\n\n\n\nReturn a list of \nN\n random elements of the list \nList\n.\n\n\n{choose, \nList\n}\n is equivalent to \n{choose, 1, \nList\n}\n.\n\n\nround_robin\n\n\n{round_robin, \nList\n}\n\n\n\n\nPick the next element of the list. When the last one is picked, start over from the first one.\n\n\nLogging\n\n\ndump\n\n\n{dump, \nText\n}\n\n\n\n\nWrite \nText\n to the benchmark log.\n\n\nsprintf\n\n\n{sprintf, \nFormat\n, [\nValue1\n, \nValue2\n, ...]}\n\n\n\n\nReturn \nformatted text\n with a given format and placeholder values.\n\n\nData Conversion\n\n\nt\n\n\n{t, \nList\n}\n\n\n\n\nConvert \nList\n to a tuple.\n\n\nterm_to_binary\n\n\n{term_to_binary, \nterm\n}\n\n\n\n\nConvert an Erlang term to a binary object. \nLearn more\n in the Erlang docs.\n\n\nPause\n\n\nwait\n\n\n{wait, \nTime\n}\n\n\n\n\nPause the current job for \nTime\n.", 
            "title": "Scenarios"
        }, 
        {
            "location": "/scenarios/#directives", 
            "text": "Directives  prepare the system for the benchmark and clean it up after it. This includes installing an external  worker  on test nodes, registering resource files, checking conditions, and executing shell commands before and after the test.  make_install  {make_install, [{git,  URL }, {branch,  Branch }, {dir,  Dir }]}`  Install an external worker from a remote git repository on the test nodes before running the benchmark.  MZBench downloads the worker and builds a .tgz archive, which is then distributed among the nodes and used in future provisions.  The following actions are executed during  make_install :  $ git clone  URL  temp_dir\n$ cd temp_dir\n$ git checkout  Branch \n$ cd  Dir \n$ make generate_tgz  If  branch  is not specified, the default git branch is used.  If  dir  is not specified,  .  is used.  include_resource  {include_resource,  ResourceName ,  FileName ,  Type }\n{include_resource,  ResourceName ,  FileURL ,  Type }`  Register a  resource file  as  ResourceName .  If the file is on your local machine, put it in the same directory where you invoke  mzbench run .  Type  is one of the following atoms:   text  Plain text file, interpreted as a single string.  json  JSON file. Lists are interpreted as  Erlang lists , objects are interpreted as  Erlang maps .  tsv  File with tabulation separated values, interpreted as a list of lists.  erlang  Erlang source file, interpreted directly as an  Erlang term .  binary  Custom binary (image, executable, archive, etc.), not interpreted.   pre_hook and post_hook  {pre_hook,  Actions }\n{post_hook,  Actions }  Run actions before and after the benchmark. Two kinds of actions are supported:  exec commands  and  worker calls :  Actions = [Action]\nAction = {exec, Target, BashCommand}\n    | {worker_call, WorkerMethod, WorkerModule}\n    | {worker_call, WorkerMethod, WorkerModule, WorkerType}\nTarget = all | director  Exec commands  let you to run any shell command on all nodes or only on the director node.  Worker calls  are functions defined by the worker. They can be executed only on the director node. Worker calls are used to update the  environment variables  used in the benchmark.  assert  {assert, always,  Condition }\n{assert,  Time ,  Condition }  Check if the condition  Condition  is satisfied throughout the entire benchmark or at least for the amount of time  Time .   Time  is a tuple  { Duration , (ms|sec|min|h)} , e.g.  {1, sec} .  Condition  is a comparison of two value and is defined as a tuple  { Operation ,  Operand1 ,  Operand2 } .  Operation  is one of four atoms:   lt  Less than.  gt  Greater than.  lte  Less than or equal to.  gte  Greater than or equal to.   Operand1  and  Operand2  are the values to compare. They can be integers, floats, or  metrics  values.  Metrics  are numerical values collected by the worker during the benchmark. To get the metric value, put its name between double quotation marks:  {gt,  http_ok , 20}  The  http_ok  metric is provided by the  simple_http  worker. This condition passes if the number of successful HTTP responses is greater than 20.", 
            "title": "Directives"
        }, 
        {
            "location": "/scenarios/#pools", 
            "text": "Pool  represents a sequence of  jobs \u2014statements to run. The statements are defined by the  worker  and  MZBench s standard library . The jobs are evenly distributed between nodes, so they can be executed in parallel.  Here s a pool that sends HTTP GET requests to two sites on 10 nodes in parallel:      [ {pool,\n        [ {size, 10}, {worker_type, simple_http} ],\n        [\n            {get,  http://example.com },\n            {get,  http://foobar.com } \n        ]\n    } ].  The  get  statement is provided by the built-in  simple_http  worker.  The first param in the  pool  statement is a list of  pool options .", 
            "title": "Pools"
        }, 
        {
            "location": "/scenarios/#pool-options", 
            "text": "size  required  {size,  NumberOfJobs }`  How many times you want the pool executed.  If there s enough nodes and  worker_start  is not set, MZBench will start the jobs simultaneously and run them in parallel.  NumberOfJobs  is any positive number.  worker_type  required  {worker_type,  WorkerName }  The worker that provides statements for the jobs.   Hint  A pool uses exactly one worker. If you need multiple workers in the benchmark, just write a pool for each one.   worker_start  {worker_start, {linear,  Rate }}\n{worker_start, {poisson,  Rate }}\n{worker_start, {exp,  Scale ,  Time }}\n{worker_start, {pow,  Exponent ,  Scale ,  Time }}  Start the jobs with a given rate:   linear  Constant rate  Rate , e.g. 10 per minute.    Rate  is a tuple  { N , (rps|rpm|rph)} , e.g.  {10, rps} . It means  N  jobs per second, minute, or hour.   poisson  Rate defined by a  Poisson process  with \u03bb =  Rate .  exp   Start jobs with  exponentially growing  rate with the scale factor  Scale :  Scale \u00d7 e Time   pow   Start jobs with rate growing as a  power function  with the exponent  Exponent  and the scale factor  Scale :  Scale \u00d7 Time Exponent    You can customize and combine rates:  think_time  {think_time,  Time ,  Rate }  Start jobs with rate  Rate  for a second, then sleep for  Time  and repeat.  ramp  {ramp, linear,  StartRate ,  EndRate }  Linearly change the rate from  StartRate  at the beginning of the pool to  EndRate  at its end.  comb  {comb,  Rate1 ,  Time1 ,  Rate2 ,  Time2 , ...}  Start jobs with rate  Rate1  for  Time1 , then switch to  Rate2  for  Time2 , etc.", 
            "title": "Pool Options"
        }, 
        {
            "location": "/scenarios/#loops", 
            "text": "Loop  is a sequence of statements executed over and over for a given time.  A loop looks similar to a  pool \u2014it consists of a list of  options  and a list statements to run:  {loop, [\n        {time,  Time },\n        {rate,  Rate },\n        {parallel,  N },\n        {iterator,  Name },\n        {spawn,  Spawn }\n    ],\n    [\n         Statement1 ,\n         Statement2 ,\n        ...\n    ]\n}  Here s a loop that sends HTTP GET requests for 30 seconds with a growing rate of 1 \u2192 5 rps:  {loop, [\n        {time, {30, sec}},\n        {rate, {ramp, linear, {1, rps}, {5, rps}}}\n    ],\n    [\n        {get,  http://example.com }\n    ]\n}  You can put loops inside loops. Here s a nested loop that sends HTTP GET requests for 30 seconds, increasing the rate by 1 rps every three seconds:  {loop, [\n        {time, {30, sec}},\n        {rate, {10, rpm}},\n        {iterator,  i }\n    ],\n    [\n        {loop, [\n                {time, {3, sec}}, \n                {rate, {{var,  i }, rps}}\n            ],\n            [\n                {get,  http://google.com }\n            ]\n        }\n    ]\n}  The difference between these two examples is that in the first case the rate is growing smoothly and in the second one it s growing in steps.", 
            "title": "Loops"
        }, 
        {
            "location": "/scenarios/#loop-options", 
            "text": "time  required  {time,  Time }  Run the loop for  Time .  rate  {rate,  Rate }  Repeat the loop with the  Rate  rate.  parallel  {parallel,  N }  Run  N  iterations of the loop in parallel.  iterator  {iterator,  IterName }  Define a variable named  IterName  inside the loop that contains the current iteration number. It can be accessed with  {var,  IterName } .  spawn  {spawn, (true|false)}  If  true , every iteration runs in a separate, spawned process. Default is  false .", 
            "title": "Loop options"
        }, 
        {
            "location": "/scenarios/#resource-files", 
            "text": "Resource file  is an external data source for the benchmark.  To declare a resource file for the benchmark, use  include_resource .  Once the resource file is registered, its content can be included at any place in the scenario using the  resource  statement:  {resource,  ResourceName } .  For example, suppose we have a file  names.json :  [\n    \"Bob\",\n    \"Alice\",\n    \"Guido\"\n]  Here s how you can use this file in a scenario:  [\n    {include_resource, names,  names.json , json},\n    {pool, [\n            {size, 3},\n            {worker_type, dummy_worker}\n        ],\n        [\n            {loop, [\n                    {time, {5, sec}},\n                    {rate, {1, rps}}\n                ],\n                [\n                    {print, {choose, {resource, names}}} % print a random name from the file\n                ]\n            }\n        ]\n    }\n].", 
            "title": "Resource Files"
        }, 
        {
            "location": "/scenarios/#standard-library", 
            "text": "", 
            "title": "Standard Library"
        }, 
        {
            "location": "/scenarios/#environment-variables", 
            "text": "Environment variables  are global values that can be accessed at any point of the benchmark. They are useful to store the benchmark global state like its total duration, or global params like the execution speed.  To set an environment variable, call  mzbench  with the  --env  param:  $ ./bin/mzbench run --env foo=bar --env n=42  var  {var,  VarName }\n{var,  VarName ,  DefaultValue }  To get the value of a variable, refer to it by the name:  {var, \" VarName \"} .  {var,  foo } % returns  bar \n{var,  n } % returns  42 , a string  If you refer to an undefined variable, the benchmark crashes. You can avoid this by setting a default value for the variable:  {var, \" VarName \",  DefaultValue } :  {var,  anothervar ,  Foo } % returns  Foo  if anothervar is not set  numvar  {numvar,  VarName }\n{numvar,  VarName ,  DefaultValue }  By default, variable values are considered strings. To get a numerical value (integer or float), use  {numvar, \"VarName\"} :  {numvar,  n } % returns 42, an integer.", 
            "title": "Environment Variables"
        }, 
        {
            "location": "/scenarios/#parallelization-and-syncing", 
            "text": "parallel  {parallel,  Statement1 ,  Statement2 , ...}  Execute multiple statements in parallel. Unlike executing statements in a pool, this way all statements are executed on the same node.  set_signal  {set_signal,  SignalName }\n{set_signal,  SignalName ,  Count ]}  Emit a global signal  SignalName .  If  Count  is specified, the signal is emitted  Count  times.  SignalName  is a string, atom, number, or, in fact, any  Erlang term .  wait_signal  {wait_signal,  SignalName }\n{wait_signal,  SignalName ,  Count }  Wait for the global signal  SignalName  to be emitted. If  Count  is specified, wait for the signal to be emitted  Count  times.", 
            "title": "Parallelization and Syncing"
        }, 
        {
            "location": "/scenarios/#errors-handling", 
            "text": "ignore_failure  {ignore_failure,  Statement }  Execute the statement  Statement  and continue with the benchmark even if it fails.  If the statement succeeds, its result is returned; otherwise, the failure reason is returned.", 
            "title": "Errors Handling"
        }, 
        {
            "location": "/scenarios/#randomization", 
            "text": "random_number  {random_number,  Min ,  Max }\n{random_number,  Max }  Return a random number between  Min  and  Max , including  Min  and not including  Max .  {random_number,  Max }  is equivalent to  {random_number, 0,  Max }  random_list  {random_list,  Size }  Return a list of random integer of length  Size .  random_binary  {random_binary,  Size }  Return a binary sequence of  Size  random bytes.  choose  {choose,  N ,  List }\n{choose,  List }  Return a list of  N  random elements of the list  List .  {choose,  List }  is equivalent to  {choose, 1,  List } .  round_robin  {round_robin,  List }  Pick the next element of the list. When the last one is picked, start over from the first one.", 
            "title": "Randomization"
        }, 
        {
            "location": "/scenarios/#logging", 
            "text": "dump  {dump,  Text }  Write  Text  to the benchmark log.  sprintf  {sprintf,  Format , [ Value1 ,  Value2 , ...]}  Return  formatted text  with a given format and placeholder values.", 
            "title": "Logging"
        }, 
        {
            "location": "/scenarios/#data-conversion", 
            "text": "t  {t,  List }  Convert  List  to a tuple.  term_to_binary  {term_to_binary,  term }  Convert an Erlang term to a binary object.  Learn more  in the Erlang docs.", 
            "title": "Data Conversion"
        }, 
        {
            "location": "/scenarios/#pause", 
            "text": "wait  {wait,  Time }  Pause the current job for  Time .", 
            "title": "Pause"
        }, 
        {
            "location": "/server_api/", 
            "text": "MZBench server API\n\n\nAPI allows driving bench execution and gathering metrics. It can\nt execute scripts locally or validate because these functions are not remote. As other HTTP APIs it has some endpoints and POST/GET parameters. Please notice that if log/metric compression is enabled at server.config \n you need to be able to handle compressed answers from MZBench server.\n\n\nMost of the requests return JSON objects, in case of an error it has \nreason\n \n error text and \nreason_code\n \n short error textual id.\n\n\nAPI requests can be debugged with curl utility, for example:\n\n\n$ curl http://mzbench.myserver.com/logs?id=777\n{\"reason_code\":\"not_found\",\"reason\":\"Benchmark 777 is not found\"}\n\n$ curl http://mzbench.myserver.com/status?id=1236\n{\"status\":\"complete\",\"start_time\":\"2015-02-10T21:10:34Z\",\"finish_time\":\"2015-02-10T22:17:06Z\"}\n\n\n\nEndpoints\n\n\nPOST /start?nodes=\n\n\nAsyncronous bench start, the only required POST parameter is file to be executed, it should be named \nbench\n, follow \nDSL Reference\n to learn how to write such files.\n\n\nOptional query parameters:\n\n\n\n\nnodes \n the number of nodes to be allocated or a comma-separated list of nodes to be used (for pre-allocated cluster), default is 1.\n\n\nnode_commit \n commit or branch to be used for node. It is useful when you need to run different versions of the node with one server.\n\n\ndeallocate_after_bench \n whether nodes require to be deallocated after the execution has finished. This option can be \ntrue\n or \nfalse\n, default is \ntrue\n.\n\n\nprovision_nodes \n whether software installation is required for nodes. In some cases, you already have MZBench installed and you do not need to reinstall it, default is \ntrue\n.\n\n\nbenchmark_name \n set benchmark name.\n\n\n\n\nIn a case of success, response is JSON object contains \nid\n and \nstatus\n fields.\n\n\nExamples:\n\n\n# start scenario.erl on five cloud nodes\ncurl -XPOST --form bench=@scenario.erl http://mzbench.myserver.com/start?nodes=5\n{\"status\":\"pending\",\"id\":122}\n\n# start scenario.erl on five preallocated nodes (n1 .. n5)\ncurl -XPOST --form bench=@scenario.erl http://mzbench.myserver.com/start?nodes=n1,n2,n3,n4,n5\n\n# start scenario.erl using node from a personal branch\ncurl -XPOST --form bench=@scenario.erl http://mzbench.myserver.com/start?node_commit=my_branch\n\n# start scenario.erl and disable cloud nodes deallocation for debugging purposes\ncurl -XPOST --form bench=@scenario.erl http://mzbench.myserver.com/start?deallocate_after_bench\n\n# start scenario.erl using node from a personal branch\ncurl -XPOST --form bench=@scenario.erl http://mzbench.myserver.com/start?node_commit=my_branch\n\n\n\nGET /status?id=number\n\n\nRequest bench status, response fields are \nstatus\n, \nstart_time\n and \nfinish_time\n.\n\n\nExample:\n\n\ncurl 'http://mzbench.myserver.com/status?id=122'\n{\"status\":\"complete\",\"start_time\":\"2015-08-12T07:25:42Z\",\"finish_time\":\"2015-08-12T07:26:29Z\"}\n\n\n\nGET /stop?id=number\n\n\nStop requested benchmark, in a case of success, response JSON object will contain the only field \nstatus\n.\n\n\nGET /restart?id=number\n\n\nClone and start one of the previously executes scenarios. The only parameter is an id of a bench to be cloned. Response is similar to \n/start\n function.\n\n\nGET /logs?id=number\n\n\nRequest benchmark logs, in a case of success, response will be plain bench log text. If the bench is still running, log will be streamed continuously until it finishes.\n\n\nGET /data?id=number\n\n\nRequest benchmark data, in a case of success, response will be tab-delimited CSV with timestamp followed by metric name and value, for example:\n\n\ncurl http://mzbench.myserver.com/data?id=1236\n1439245024  mzb.workers.failed.value    0\n1439245024  mzb.workers.failed.rps.value    0.0\n1439245024  mzb.workers.started.value   2000\n\n\n\nMetric data for running benches is streamed in the same manner as logs.\n\n\nConventions\n\n\n\n\nBench id is a non-negative integer\n\n\nStatus is a string, allowed values are: \npending\n, \nrunning\n, \ncomplete\n, \nfailed\n, \nstopped\n\n\nDate is a string in ISO 8601 format, like this: \n2015-08-12T07:25:42Z", 
            "title": "Server API"
        }, 
        {
            "location": "/server_api/#mzbench-server-api", 
            "text": "API allows driving bench execution and gathering metrics. It can t execute scripts locally or validate because these functions are not remote. As other HTTP APIs it has some endpoints and POST/GET parameters. Please notice that if log/metric compression is enabled at server.config   you need to be able to handle compressed answers from MZBench server.  Most of the requests return JSON objects, in case of an error it has  reason    error text and  reason_code    short error textual id.  API requests can be debugged with curl utility, for example:  $ curl http://mzbench.myserver.com/logs?id=777\n{\"reason_code\":\"not_found\",\"reason\":\"Benchmark 777 is not found\"}\n\n$ curl http://mzbench.myserver.com/status?id=1236\n{\"status\":\"complete\",\"start_time\":\"2015-02-10T21:10:34Z\",\"finish_time\":\"2015-02-10T22:17:06Z\"}", 
            "title": "MZBench server API"
        }, 
        {
            "location": "/server_api/#endpoints", 
            "text": "POST /start?nodes=  Asyncronous bench start, the only required POST parameter is file to be executed, it should be named  bench , follow  DSL Reference  to learn how to write such files.  Optional query parameters:   nodes   the number of nodes to be allocated or a comma-separated list of nodes to be used (for pre-allocated cluster), default is 1.  node_commit   commit or branch to be used for node. It is useful when you need to run different versions of the node with one server.  deallocate_after_bench   whether nodes require to be deallocated after the execution has finished. This option can be  true  or  false , default is  true .  provision_nodes   whether software installation is required for nodes. In some cases, you already have MZBench installed and you do not need to reinstall it, default is  true .  benchmark_name   set benchmark name.   In a case of success, response is JSON object contains  id  and  status  fields.  Examples:  # start scenario.erl on five cloud nodes\ncurl -XPOST --form bench=@scenario.erl http://mzbench.myserver.com/start?nodes=5\n{\"status\":\"pending\",\"id\":122}\n\n# start scenario.erl on five preallocated nodes (n1 .. n5)\ncurl -XPOST --form bench=@scenario.erl http://mzbench.myserver.com/start?nodes=n1,n2,n3,n4,n5\n\n# start scenario.erl using node from a personal branch\ncurl -XPOST --form bench=@scenario.erl http://mzbench.myserver.com/start?node_commit=my_branch\n\n# start scenario.erl and disable cloud nodes deallocation for debugging purposes\ncurl -XPOST --form bench=@scenario.erl http://mzbench.myserver.com/start?deallocate_after_bench\n\n# start scenario.erl using node from a personal branch\ncurl -XPOST --form bench=@scenario.erl http://mzbench.myserver.com/start?node_commit=my_branch  GET /status?id=number  Request bench status, response fields are  status ,  start_time  and  finish_time .  Example:  curl 'http://mzbench.myserver.com/status?id=122'\n{\"status\":\"complete\",\"start_time\":\"2015-08-12T07:25:42Z\",\"finish_time\":\"2015-08-12T07:26:29Z\"}  GET /stop?id=number  Stop requested benchmark, in a case of success, response JSON object will contain the only field  status .  GET /restart?id=number  Clone and start one of the previously executes scenarios. The only parameter is an id of a bench to be cloned. Response is similar to  /start  function.  GET /logs?id=number  Request benchmark logs, in a case of success, response will be plain bench log text. If the bench is still running, log will be streamed continuously until it finishes.  GET /data?id=number  Request benchmark data, in a case of success, response will be tab-delimited CSV with timestamp followed by metric name and value, for example:  curl http://mzbench.myserver.com/data?id=1236\n1439245024  mzb.workers.failed.value    0\n1439245024  mzb.workers.failed.rps.value    0.0\n1439245024  mzb.workers.started.value   2000  Metric data for running benches is streamed in the same manner as logs.", 
            "title": "Endpoints"
        }, 
        {
            "location": "/server_api/#conventions", 
            "text": "Bench id is a non-negative integer  Status is a string, allowed values are:  pending ,  running ,  complete ,  failed ,  stopped  Date is a string in ISO 8601 format, like this:  2015-08-12T07:25:42Z", 
            "title": "Conventions"
        }, 
        {
            "location": "/deployment_guide/", 
            "text": "Introduction\n\n\nThis document explains how to deploy and configure an MZBench API server on your own infrastructure.\nThe scope includes installation and configuration of the API server. Installation\nand configuration of the Graphite host is not covered. The Graphite host is\n optional, although you will certainly need it for any serious use.\n\n\nMZBench architecture\n\n\nIn its more general form, a complete MZBench installation consists of several separate hosts:\n\n\n\n\nAn MZBench API server that accepts the client requests and provides the dashboard\n\n\nA metric gathering host, usually Graphite, providing the graph plotting services\n\n\nA set of hosts to use as a worker nodes\n\n\n\n\nThe API server and Graphite hosts are permanent, while the worker nodes can be dynamically\nallocated when needed, for example on Amazon.\n\n\nServer installation\n\n\nRequirements\n\n\nTo run an MZBench API server, you need to install the following packages:\n\n\n\n\nA C++ compiler\n\n\nErlang R17 distribution (http://www.erlang.org)\n\n\nPython v2.6 (https://www.python.org)\n\n\nPIP Python package manager (https://pip.pypa.io/en/stable/)\n\n\n\n\nServer installation\n\n\nTo install the MZBench API server, perform the following commands:\n\n\n# Clone the current MZBench source code\ngit clone https://github.com/machinezone/mzbench.git\n\n# Install additional Python packages\nsudo pip install -r mzbench/requirements.txt\n\n# Start the server\n./mzbench/bin/mzbench start_server\n\n# Stop the server\n./mzbench/bin/mzbench stop_server\n\n\n\nServer configuration\n\n\nTo apply configuration changes you need to restart the server if it\ns running.\n\n\nConfiguration file format\n\n\nThe MZBench server parameters are set in configuration file.\nIt can be passed as an argument to bin/mzbench start_server command.\n\n\nFor example: bin/mzbench start_server \nconfig ./server.config\n\n\nBy default, the server tries to load configuration from the following places:\n\n\n~/.config/mzbench/server.config\n/etc/mzbench/server.config\n\n\nThe configuraion file is essentially an Erlang term. At the top level, it is a list of tuples, terminated by the dot.\n Each tuple represent a configuration category. Its first element is an atom identifying the\n category, its second element is a list of actual parameters. Each of parameters is a tuple\n  by itself, containing the parameter name and the value.\n\n\nFor the sake of clarity, let\ns see the following example configuration:\n\n\n[\n    {mzbench_api, [\n        {graphite, \"172.21.8.192\"},\n        {listen_port, 80}\n    ]}\n].\n\n\n\nIt contains only one category: \nmzbench_api\n. This category is used for the parameters of the server itself.\n\n\nHere, we specify two parameters: \ngraphite\n and \nlisten_port\n. \ngraphite\n specifies the IP\n address of the host containing the Graphite server to use (here \n172.21.8.192\n). \nlisten_port\n\n specifies which port should be used to access the server dashboard (here \n80\n).\n\n\nServer parameters (\nmzbench_api\n)\n\n\nThese parameters are going to the \nmzbench_api\n category.\n\n\n{cloud_plugins, [{Name :: atom(), Opts :: #{}}]}\n\n\nPlugins responsible for node allocation, [{dummy, #{module =\n mzb_dummycloud_plugin}}] by default.\nName represents the name of the particular instance of plugin and could be any atom.\nOpts must contain either module or application key, other keys in Opts depend on particular plugin type.\nIt is possible to specify several plugins. The actually used plugin can be specified with \ncloud option of the bin/mzbench utility.\nThere are three cloud plugins available in mzbench so far. See below for configuration details and a few examples.\nSee also: \nCloud plugin creation guide\n\n\nConfiguration of the AWS EC2 cloud plugin\n\n\nmzb_api_ec2_plugin module allocates hosts in EC2 and requires \ninstance_spec\n and \nconfig\n keys to be specified in Opts (see above).\nFor AWS specific configuration details, see \nerlcloud documentation\n.\nAWS image tips: an image should contain Erlang R17, gcc, gcc-c++, git, sudo.\nsudo shoud be available for non-tty execution (put \nDefaults !requiretty\n to /etc/sudoers).\nIt is also required to have ssh and tcp 4801/4802 ports connectivity between server and nodes for\nlogs and metrics. Please refer EC2 documentation on image publish process, it can be done from\nweb AWS console within few clicks. We prepared such image with Amazon Linux (ami-3b90a80b), but you\nmay require some additional software. For this image you need to register and specify your keypair name\nwith your AWS credentials.\n\n\nConfiguration example:\n\n\n{cloud_plugins, [{ec2, #{module =\n mzb_api_ec2_plugin,\n                         instance_spec =\n [\n                          {image_id, \"ami-2c03b22c\"},\n                          {group_set, \"\"},\n                          {key_name, \"-\"},\n                          {subnet_id, \"-\"},\n                          {instance_type, \"t2.micro\"},\n                          {availability_zone, \"us-west-2a\"}\n                        ],\n                        config =\n [\n                          {ec2_host, \"ec2.us-west-2.amazonaws.com\"},\n                          {access_key_id, \"-\"},\n                          {secret_access_key, \"-\"}\n                         ]\n                        instance_user =\n \"ec2-user\",\n                    }}]},\n\n\n\nConfiguration of static cloud plugin\n\n\nmzb_staticcloud_plugin does not allocate any hosts, the specified list of hosts is used instead.\n\n\nConfiguration example:\n\n\n{cloud_plugins, [{static, #{module =\n mzb_staticcloud_plugin,\n                           hosts =\n [\"host1\", \"host2\"]\n                           }}]}\n\n\n\nConfiguration of dummy cloud plugin\n\n\nmzb_dummycloud_plugin does not allocate any hosts, localhost is used instead.\n\n\nConfiguration example:\n\n\n{cloud_plugins, [{dummy, #{module =\n mzb_dummycloud_plugin}}]}\n\n\n\n{bench_data_dir, \"\npath\n\"}\n\n\nThis parameter specifies where to store the various benchmark generated data.\n\n\nBy default \nmzbench_api_data\n in the home directory of the user who started the server.\n\n\n{graphite, \"\nhostname or ip\n\"}\n\n\nThis parameter specifies where to send the metrics, the host or IP address of the Graphite server.\n\n\n{graphite_api_key, \"\nstring\n\"}\n\n\nHostedgraphite.com API key.\n\n\n{graphite_url, \"\nURL\n\"}\n\n\nHostedgraphite.com URL, usually something like this: https://www.hostedgraphite.com/xxxxxxxx/graphite/\n\n\nBy default, no metrics will be sent.\n\n\n{listen_port, \nport\n}\n\n\nThis parameter is used to specify the port used to access the server dashboard.\n\n\nBy default, the dashboard listens on \n4800\n.\n\n\n{network_interface, \"\nip address\n\"}\n\n\nThis parameter is used to specify the interface for dashboard to listen (by default it is \n127.0.0.1\n),\nso the dashboard won\nt be available for any external connections. To open dashboard\n for everyone, specify \n0.0.0.0\n\n \nWarning:\n mzbench doesn\nt provide any authentication and opening it\nto everyone would bring additional vulnerability to your server, mzbench dashboard should be protected with an external auth proxy like nginx.\n\n\n{node_git, \"\nurl\n\"}\n\n\nSpecifies the MZBench Git repository used to deploy the worker nodes.\n\n\nBy default, the MZBench source code will be taken from \nhttps://github.com/machinezone/mzbench.git\n.\n\n\n{node_commit, \"\nstring\n\"}\n\n\nSpecifies Git commit SHA or Git branch name used to deploy the worker nodes.\n\n\n{ntp_max_timediff, \nfloat\n}\n\n\nMaximum distance between node timers (default is 0.1), this check is optional and would only print a warning if failed.", 
            "title": "Deployment"
        }, 
        {
            "location": "/deployment_guide/#introduction", 
            "text": "This document explains how to deploy and configure an MZBench API server on your own infrastructure.\nThe scope includes installation and configuration of the API server. Installation\nand configuration of the Graphite host is not covered. The Graphite host is\n optional, although you will certainly need it for any serious use.", 
            "title": "Introduction"
        }, 
        {
            "location": "/deployment_guide/#mzbench-architecture", 
            "text": "In its more general form, a complete MZBench installation consists of several separate hosts:   An MZBench API server that accepts the client requests and provides the dashboard  A metric gathering host, usually Graphite, providing the graph plotting services  A set of hosts to use as a worker nodes   The API server and Graphite hosts are permanent, while the worker nodes can be dynamically\nallocated when needed, for example on Amazon.", 
            "title": "MZBench architecture"
        }, 
        {
            "location": "/deployment_guide/#server-installation", 
            "text": "", 
            "title": "Server installation"
        }, 
        {
            "location": "/deployment_guide/#requirements", 
            "text": "To run an MZBench API server, you need to install the following packages:   A C++ compiler  Erlang R17 distribution (http://www.erlang.org)  Python v2.6 (https://www.python.org)  PIP Python package manager (https://pip.pypa.io/en/stable/)", 
            "title": "Requirements"
        }, 
        {
            "location": "/deployment_guide/#server-installation_1", 
            "text": "To install the MZBench API server, perform the following commands:  # Clone the current MZBench source code\ngit clone https://github.com/machinezone/mzbench.git\n\n# Install additional Python packages\nsudo pip install -r mzbench/requirements.txt\n\n# Start the server\n./mzbench/bin/mzbench start_server\n\n# Stop the server\n./mzbench/bin/mzbench stop_server", 
            "title": "Server installation"
        }, 
        {
            "location": "/deployment_guide/#server-configuration", 
            "text": "To apply configuration changes you need to restart the server if it s running.", 
            "title": "Server configuration"
        }, 
        {
            "location": "/deployment_guide/#configuration-file-format", 
            "text": "The MZBench server parameters are set in configuration file.\nIt can be passed as an argument to bin/mzbench start_server command.  For example: bin/mzbench start_server  config ./server.config  By default, the server tries to load configuration from the following places:  ~/.config/mzbench/server.config\n/etc/mzbench/server.config  The configuraion file is essentially an Erlang term. At the top level, it is a list of tuples, terminated by the dot.\n Each tuple represent a configuration category. Its first element is an atom identifying the\n category, its second element is a list of actual parameters. Each of parameters is a tuple\n  by itself, containing the parameter name and the value.  For the sake of clarity, let s see the following example configuration:  [\n    {mzbench_api, [\n        {graphite, \"172.21.8.192\"},\n        {listen_port, 80}\n    ]}\n].  It contains only one category:  mzbench_api . This category is used for the parameters of the server itself.  Here, we specify two parameters:  graphite  and  listen_port .  graphite  specifies the IP\n address of the host containing the Graphite server to use (here  172.21.8.192 ).  listen_port \n specifies which port should be used to access the server dashboard (here  80 ).", 
            "title": "Configuration file format"
        }, 
        {
            "location": "/deployment_guide/#server-parameters-mzbench_api", 
            "text": "These parameters are going to the  mzbench_api  category.  {cloud_plugins, [{Name :: atom(), Opts :: #{}}]}  Plugins responsible for node allocation, [{dummy, #{module =  mzb_dummycloud_plugin}}] by default.\nName represents the name of the particular instance of plugin and could be any atom.\nOpts must contain either module or application key, other keys in Opts depend on particular plugin type.\nIt is possible to specify several plugins. The actually used plugin can be specified with  cloud option of the bin/mzbench utility.\nThere are three cloud plugins available in mzbench so far. See below for configuration details and a few examples.\nSee also:  Cloud plugin creation guide  Configuration of the AWS EC2 cloud plugin  mzb_api_ec2_plugin module allocates hosts in EC2 and requires  instance_spec  and  config  keys to be specified in Opts (see above).\nFor AWS specific configuration details, see  erlcloud documentation .\nAWS image tips: an image should contain Erlang R17, gcc, gcc-c++, git, sudo.\nsudo shoud be available for non-tty execution (put  Defaults !requiretty  to /etc/sudoers).\nIt is also required to have ssh and tcp 4801/4802 ports connectivity between server and nodes for\nlogs and metrics. Please refer EC2 documentation on image publish process, it can be done from\nweb AWS console within few clicks. We prepared such image with Amazon Linux (ami-3b90a80b), but you\nmay require some additional software. For this image you need to register and specify your keypair name\nwith your AWS credentials.  Configuration example:  {cloud_plugins, [{ec2, #{module =  mzb_api_ec2_plugin,\n                         instance_spec =  [\n                          {image_id, \"ami-2c03b22c\"},\n                          {group_set, \"\"},\n                          {key_name, \"-\"},\n                          {subnet_id, \"-\"},\n                          {instance_type, \"t2.micro\"},\n                          {availability_zone, \"us-west-2a\"}\n                        ],\n                        config =  [\n                          {ec2_host, \"ec2.us-west-2.amazonaws.com\"},\n                          {access_key_id, \"-\"},\n                          {secret_access_key, \"-\"}\n                         ]\n                        instance_user =  \"ec2-user\",\n                    }}]},  Configuration of static cloud plugin  mzb_staticcloud_plugin does not allocate any hosts, the specified list of hosts is used instead.  Configuration example:  {cloud_plugins, [{static, #{module =  mzb_staticcloud_plugin,\n                           hosts =  [\"host1\", \"host2\"]\n                           }}]}  Configuration of dummy cloud plugin  mzb_dummycloud_plugin does not allocate any hosts, localhost is used instead.  Configuration example:  {cloud_plugins, [{dummy, #{module =  mzb_dummycloud_plugin}}]}  {bench_data_dir, \" path \"}  This parameter specifies where to store the various benchmark generated data.  By default  mzbench_api_data  in the home directory of the user who started the server.  {graphite, \" hostname or ip \"}  This parameter specifies where to send the metrics, the host or IP address of the Graphite server.  {graphite_api_key, \" string \"}  Hostedgraphite.com API key.  {graphite_url, \" URL \"}  Hostedgraphite.com URL, usually something like this: https://www.hostedgraphite.com/xxxxxxxx/graphite/  By default, no metrics will be sent.  {listen_port,  port }  This parameter is used to specify the port used to access the server dashboard.  By default, the dashboard listens on  4800 .  {network_interface, \" ip address \"}  This parameter is used to specify the interface for dashboard to listen (by default it is  127.0.0.1 ),\nso the dashboard won t be available for any external connections. To open dashboard\n for everyone, specify  0.0.0.0 \n  Warning:  mzbench doesn t provide any authentication and opening it\nto everyone would bring additional vulnerability to your server, mzbench dashboard should be protected with an external auth proxy like nginx.  {node_git, \" url \"}  Specifies the MZBench Git repository used to deploy the worker nodes.  By default, the MZBench source code will be taken from  https://github.com/machinezone/mzbench.git .  {node_commit, \" string \"}  Specifies Git commit SHA or Git branch name used to deploy the worker nodes.  {ntp_max_timediff,  float }  Maximum distance between node timers (default is 0.1), this check is optional and would only print a warning if failed.", 
            "title": "Server parameters (mzbench_api)"
        }, 
        {
            "location": "/workers/", 
            "text": "Today internet is a huge set of ever changing technologies. It would be impossible in practice for MZBench distribution to provide an extensive library of functions to access all the possible services and protocols. Instead, it uses a plugin system called \nworkers\n.\n\n\nA \nworker\n is an Erlang application providing a set of MZBench DSL statements to access a particular service (such as HTTP or SSH server) and collecting a set of statistics about its usage. A small set of standard \nworkers\n is provided with the distribution, but you will likely need to write your own to access the service you want to benchmark. This document is here to guide you through this process.\n\n\nBecause a MZBench \nworker\n is an Erlang application, you need some basic knowledge of this programming language to understand this document. You can refer yourself to \nGetting Started with Erlang User\ns Guide\n or to the \nLearn You Some Erlang for great good!\n book for an introduction to the matter.\n\n\nHow to Write a Worker\n\n\nCommand line utilities\n\n\nMZBench distribution provide some command line utilities to assist you during your \nworker\n development effort.\n\n\nNew worker generation\n\n\nFirst of all, you can generate an empty \nworker\n application using the following command (here and later \nMZBENCH_SRC\n means the path to the MZBench source code):\n\n\nMZBENCH_SRC\n/bin/mzbench new_worker \nworker_name\n\n\n\n\nThis will create a new directory \nworker_name\n containing a minimalistic, but fully functional MZBench \nworker\n named \nworker_name\n. Particularly interesting generated files are \nsrc/\nworker_name\n.erl\n containing the \nworker\n source code and \nexamples/\nworker_name\n.erl\n containing a simple MZBench scenario using it.\n\n\nIf the service you want to access is based on some well known protocol, such as TCP, the \nnew_worker\n command can generate your a more elaborate \nworker\n already containing the usual boilerplate code for this type of service. You can obtain a list of available protocols by executing:\n\n\nMZBENCH_SRC\n/bin/mzbench list_templates\n\n\n\nThen generate your \nworker\n by adding an additional parameter to the \nnew_worker\n command:\n\n\nMZBENCH_SRC\n/bin/mzbench new_worker --template \nprotocol\n \nworker_name\n\n\n\n\nWorker compilation and debugging\n\n\nDuring any serious development, you will certainly need a to do a lot of debugging. If every time you want to launch your test scenario you would need to commit your \nworker\n to a remote Git repository and to launch a complete benchmark, it would be cumbersome. So MZBench provide you with a way to quickly build your worker and launch a local instance of your benchmarking scenario using it.\n\n\nInside your \nworker\n source code directory (the root one, not the \nsrc\n), execute the following command replacing \nscript\n by the path to the benchmarking scenario you want to run:\n\n\nMZBENCH_SRC\n/bin/mzbench run_local \nscript\n\n\n\n\nYou can pass the environment variables using the \n--env\n option. But, please note that all \nmake_install\n top-level statements will be ignored in this execution mode.\n\n\nWorker execution\n\n\nAfter you have done with debugging, you need to execute your worker in a cloud. To do that, you need to specify worker git address at your benchmark script with \n{make_install, [{git, \nURL\n}, {branch, \nBranch\n}, {dir, \nDir\n}]}\n, \nhttp worker example\n.\n\n\nGeneral worker structure\n\n\nAn MZBench \nworker\n provides a set of DSL statements (i.e. sub-routines) and a set of metrics. The different sub-routines need not to be independent as the worker can have internal state.\n\n\nTo understand the general structure of a \nworker\n, let see the source code of the \ndummy_worker\n provided with the MZBench distribution:\n\n\n-module(dummy_worker).\n-export([initial_state/0, metrics/0,\n         print/3]).\n\n-include(\"mzb_types.hrl\").\n\n-type state() :: string().\n-type meta() :: [{Key :: atom(), Value :: any()}].\n\n-type graph_group() :: {group, Name :: string(), [graph()]}\n                     | graph().\n-type graph()       :: {graph, Opts :: #{metrics =\n [metric()],\n                                         units =\n string(),\n                                         title =\n string()}}\n                     | [metric()]\n                     | metric().\n-type metric()      :: {Name :: string(), Type :: metric_type() }\n                     | {Name :: string(), Type :: metric_type(), Opts :: map() }.\n-type metric_type() :: counter | gauge | histogram.\n\n-spec initial_state() -\n state().\ninitial_state() -\n [].\n\n-spec metrics() -\n [graph_group()].\nmetrics() -\n [{group, \"Application Metrics\", [\n                {graph, #{ title =\n \"Dummy counter\",\n                           units =\n \"budger\",\n                           metrics =\n [{\"dummy_counter\", counter}]}}\n             ]}].\n\n-spec print(state(), meta(), string()) -\n {nil, state()}.\nprint(State, Meta, Text) -\n\n    mzb_metrics:notify({\"dummy_counter\", counter}, 1),\n    lager:info(\"Printing ~p, Meta: ~p~n\", [Text, Meta]),\n    {nil, State}.\n\n\n\nAs can be seen, it export\ns 3 functions: \ninitial_state/0\n, \nmetrics/0\n and \nprint/3\n. First two of them are mandatory for any \nworker\n. \n\n\ninitial_state/0\n function can return anything and is used to initialize the \nworker\n initial state. Each parallel execution job have its own state, so this function will be called once per job start.\n\n\nmetrics/0\n function is also mandatory. It return a group of metrics generated by this \nworker\n. Please refer yourself to \nHow to define metrics\n for further reference concerning this function.\n\n\nAll the remaining exported functions defines the DSL statements provided by this \nworker\n. You can, of course, provide none, although such a \nworker\n wouldn\nt be very useful. The \ndummy_worker\n, for instance, provide the \nprint\n statement useful to output some string of characters to the standard output. Refer yourself to \nHow to define statements\n for a more detailed discussion on this topic.\n\n\nHow to define statements\n\n\nTo define a DSL statement provided by your \nworker\n you export an Erlang function that will be called when such a statement is encountered. The exported function is of the following general form:\n\n\nstatement_name\n(State, Meta, [\nParam1\n, [\nParam2\n, ...]]) -\n\n    {ReturnValue, NewState}.\n\n\n\nThe function must have the same name as the statement it defines. It must take at least two parameters: the \nworker\n internal state at the moment the statement is executed and \nmeta\n information proplist. The function can also accept any number of other parameters. They correspond to the parameters of the statement.\n\n\nThe statement function must return a tuple of two values. The first one is the return value of statement. You must return \nnil\n if your statement has no return value. The second member of the tuple is the next \nworker\n state. Each statement is processed sequentially and using previous state except internal statements of {parallel} section and iterations within a loop having parallel \n 1. In these cases statements within the same thread will share one sequence of statements and other parallel threads wont. Final state of the whole {parallel} or {loop} statement will be the very first one (from the first \nthread\n), other thread\ns states will not affect the final one.\n\n\nFor example, the following function:\n\n\nfoo(State, Meta, X, Y) -\n\n    {nil, State}.\n\n\n\nCan be called as \n{foo, X, Y}\n from a benchmarking scenario.\n\n\nMetrics\n\n\nMetrics are numerical values collected during the scenario execution. They are the main result of your \nworker\n and represent the values you want to evaluate with your benchmark.\n\n\nMetric types\n\n\nThe MZBench currently support three types of metrics:\n\n\n\n\ncounter\n - A single additive value. New values are simply added to the current one.\n\n\ngauge\n - A single non additive value. New value replaces the previous one.\n\n\nhistogram\n - A set of numerical values that quantify a distribution of values. New values are added to the distribution.\n\n\nderived\n - metric value is evaluated periodically using user defined function based on other metric values (see \nDerived metrics\n for more details).\n\n\n\n\nFor example, if you are consuming TCP packets of various sizes and you want to track overall amount of data being transferred \u2014 you need \ncounter\n and if you are interested in its distribution: mean size, 50 percentile and so on \u2014 you need a \nhistogram\n.\n\n\nDeclaring metrics\n\n\nYou declare the groups of metrics collected by your \nworker\n in the list returned by the \nmetrics/0\n function. Each group corresponds to a structure with following spec:\n\n\ngraph_group() :: {group, Name :: string(), [graph()]}\n               | graph().\ngraph()       :: {graph, Opts :: #{metrics =\n [metric()],\n                                   units =\n string(),\n                                   title =\n string()}}\n               | [metric()]\n               | metric().\nmetric()      :: {Name :: string(), Type :: metric_type() }\n               | {Name :: string(), Type :: metric_type(), Opts :: map()}.\nmetric_type() :: counter | gauge | histogram.\n\n\n\nThis structure supports a three-level hierarchy:\n\n\n\n\nGroup of graphs is placed on the top of this hierarchy. It consists of the one or more graphs and define a group of graphs under the same name.\n\n\nGraph consists of the one or more metrics that will be plotted on the same chart. Furthermore, you could specify additional option for the chart (e.g. units, title etc)\n\n\nMetric is the lowest unit of this hierarchy. It specifies the name and type of the user-defined metric.\n\n\n\n\nLet see the following metrics declaration:\n\n\nmetrics() -\n [{group, \"HTTP Requests\", [\n                {graph, #{metrics =\n [{\"success_requests\", counter}, {\"failed_requests\", counter}]}},\n                {graph, #{title =\n \"Request's latency\",\n                          units =\n \"ms\",\n                          metrics =\n [{\"latency\", histogram}]}}]}].\n\n\n\nIn this example, a group of graphs with name \nHTTP Requests\n will be created. It will consist of the several graphs presented the number of success/failed requests and the request\ns latencies. Please note then a graph could produce several charts. In the mentioned example, the graph for success and failed request will produce two charts: absolute counters and their rps.\n\n\nDerived metrics\n\n\nDerived metrics are basically gauges which are evaluated on the director node every ~10sec. To define a derived metric \nresolver\n function should be specified in metric opts dictionary. Resolver function is used to evaluate metric value.\nTypical example of the derived metrics is the current number of pending requests. We specify a function (pending_requests) to calculate the metric value in metrics options and then define the function as simple difference between number of sent requests and number of reeived responses:\n\n\nmetrics() -\n [{group, \"Requests\", [\n                {graph, #{metrics =\n [\n                    {\"requests_sent\", counter},\n                    {\"responses_received\", counter},\n                    {\"pending_requests\", derived, #{resolver =\n pending_requests}}]}},\n                ]}].\n\npending_requests() -\n\n    mzb_metrics:get_value(\"requests_sent\") - mzb_metrics:get_value(\"responses_received\").\n\n\n\nHooks\n\n\nPre and post hooks allow to run a custom code before or after benchmark. Hooks could be applied on every nodes or on the director node only. You are able to change any environment variable in your hook handler and use it in your scenario.\n\n\nScenario:\n\n\n{pre_hook, [\n    {exec, all, \"yum install zlib\"},\n    {worker_call, fetch_commit, my_worker}\n]}\n\n{pool, [{size, 3}, {worker_type, dummy_worker}], [\n    {loop, [{time, {1, sec}},\n            {rate, {ramp, linear, {10, rps}, {50, rps}}}],\n        [{print, {var, \"commit\", \"default\"}}]}]},\n\n\n\nWorker:\n\n\nfetch_commit(Env) -\n\n    {ok, [{\"commit\", \"0123456\"} | Env]}.\n\n\n\nUpdating metrics\n\n\nYou can update a declared metric from anywhere inside your \nworker\n. Simply call the following function:\n\n\nmzb_metrics:notify({\"\nmetric_name\n\", \nmetric_type\n}, \nvalue\n)\n\n\n\nThe tuple \n{\"\nmetric_name\n\", \nmetric_type\n}\n is the same that was used during the metric declaration and identifies the metric to update. The \nvalue\n is the value to add to the metric.", 
            "title": "Workers"
        }, 
        {
            "location": "/workers/#how-to-write-a-worker", 
            "text": "", 
            "title": "How to Write a Worker"
        }, 
        {
            "location": "/workers/#command-line-utilities", 
            "text": "MZBench distribution provide some command line utilities to assist you during your  worker  development effort.  New worker generation  First of all, you can generate an empty  worker  application using the following command (here and later  MZBENCH_SRC  means the path to the MZBench source code):  MZBENCH_SRC /bin/mzbench new_worker  worker_name   This will create a new directory  worker_name  containing a minimalistic, but fully functional MZBench  worker  named  worker_name . Particularly interesting generated files are  src/ worker_name .erl  containing the  worker  source code and  examples/ worker_name .erl  containing a simple MZBench scenario using it.  If the service you want to access is based on some well known protocol, such as TCP, the  new_worker  command can generate your a more elaborate  worker  already containing the usual boilerplate code for this type of service. You can obtain a list of available protocols by executing:  MZBENCH_SRC /bin/mzbench list_templates  Then generate your  worker  by adding an additional parameter to the  new_worker  command:  MZBENCH_SRC /bin/mzbench new_worker --template  protocol   worker_name   Worker compilation and debugging  During any serious development, you will certainly need a to do a lot of debugging. If every time you want to launch your test scenario you would need to commit your  worker  to a remote Git repository and to launch a complete benchmark, it would be cumbersome. So MZBench provide you with a way to quickly build your worker and launch a local instance of your benchmarking scenario using it.  Inside your  worker  source code directory (the root one, not the  src ), execute the following command replacing  script  by the path to the benchmarking scenario you want to run:  MZBENCH_SRC /bin/mzbench run_local  script   You can pass the environment variables using the  --env  option. But, please note that all  make_install  top-level statements will be ignored in this execution mode.  Worker execution  After you have done with debugging, you need to execute your worker in a cloud. To do that, you need to specify worker git address at your benchmark script with  {make_install, [{git,  URL }, {branch,  Branch }, {dir,  Dir }]} ,  http worker example .", 
            "title": "Command line utilities"
        }, 
        {
            "location": "/workers/#general-worker-structure", 
            "text": "An MZBench  worker  provides a set of DSL statements (i.e. sub-routines) and a set of metrics. The different sub-routines need not to be independent as the worker can have internal state.  To understand the general structure of a  worker , let see the source code of the  dummy_worker  provided with the MZBench distribution:  -module(dummy_worker).\n-export([initial_state/0, metrics/0,\n         print/3]).\n\n-include(\"mzb_types.hrl\").\n\n-type state() :: string().\n-type meta() :: [{Key :: atom(), Value :: any()}].\n\n-type graph_group() :: {group, Name :: string(), [graph()]}\n                     | graph().\n-type graph()       :: {graph, Opts :: #{metrics =  [metric()],\n                                         units =  string(),\n                                         title =  string()}}\n                     | [metric()]\n                     | metric().\n-type metric()      :: {Name :: string(), Type :: metric_type() }\n                     | {Name :: string(), Type :: metric_type(), Opts :: map() }.\n-type metric_type() :: counter | gauge | histogram.\n\n-spec initial_state() -  state().\ninitial_state() -  [].\n\n-spec metrics() -  [graph_group()].\nmetrics() -  [{group, \"Application Metrics\", [\n                {graph, #{ title =  \"Dummy counter\",\n                           units =  \"budger\",\n                           metrics =  [{\"dummy_counter\", counter}]}}\n             ]}].\n\n-spec print(state(), meta(), string()) -  {nil, state()}.\nprint(State, Meta, Text) - \n    mzb_metrics:notify({\"dummy_counter\", counter}, 1),\n    lager:info(\"Printing ~p, Meta: ~p~n\", [Text, Meta]),\n    {nil, State}.  As can be seen, it export s 3 functions:  initial_state/0 ,  metrics/0  and  print/3 . First two of them are mandatory for any  worker .   initial_state/0  function can return anything and is used to initialize the  worker  initial state. Each parallel execution job have its own state, so this function will be called once per job start.  metrics/0  function is also mandatory. It return a group of metrics generated by this  worker . Please refer yourself to  How to define metrics  for further reference concerning this function.  All the remaining exported functions defines the DSL statements provided by this  worker . You can, of course, provide none, although such a  worker  wouldn t be very useful. The  dummy_worker , for instance, provide the  print  statement useful to output some string of characters to the standard output. Refer yourself to  How to define statements  for a more detailed discussion on this topic.", 
            "title": "General worker structure"
        }, 
        {
            "location": "/workers/#how-to-define-statements", 
            "text": "To define a DSL statement provided by your  worker  you export an Erlang function that will be called when such a statement is encountered. The exported function is of the following general form:  statement_name (State, Meta, [ Param1 , [ Param2 , ...]]) - \n    {ReturnValue, NewState}.  The function must have the same name as the statement it defines. It must take at least two parameters: the  worker  internal state at the moment the statement is executed and  meta  information proplist. The function can also accept any number of other parameters. They correspond to the parameters of the statement.  The statement function must return a tuple of two values. The first one is the return value of statement. You must return  nil  if your statement has no return value. The second member of the tuple is the next  worker  state. Each statement is processed sequentially and using previous state except internal statements of {parallel} section and iterations within a loop having parallel   1. In these cases statements within the same thread will share one sequence of statements and other parallel threads wont. Final state of the whole {parallel} or {loop} statement will be the very first one (from the first  thread ), other thread s states will not affect the final one.  For example, the following function:  foo(State, Meta, X, Y) - \n    {nil, State}.  Can be called as  {foo, X, Y}  from a benchmarking scenario.", 
            "title": "How to define statements"
        }, 
        {
            "location": "/workers/#metrics", 
            "text": "Metrics are numerical values collected during the scenario execution. They are the main result of your  worker  and represent the values you want to evaluate with your benchmark.  Metric types  The MZBench currently support three types of metrics:   counter  - A single additive value. New values are simply added to the current one.  gauge  - A single non additive value. New value replaces the previous one.  histogram  - A set of numerical values that quantify a distribution of values. New values are added to the distribution.  derived  - metric value is evaluated periodically using user defined function based on other metric values (see  Derived metrics  for more details).   For example, if you are consuming TCP packets of various sizes and you want to track overall amount of data being transferred \u2014 you need  counter  and if you are interested in its distribution: mean size, 50 percentile and so on \u2014 you need a  histogram .  Declaring metrics  You declare the groups of metrics collected by your  worker  in the list returned by the  metrics/0  function. Each group corresponds to a structure with following spec:  graph_group() :: {group, Name :: string(), [graph()]}\n               | graph().\ngraph()       :: {graph, Opts :: #{metrics =  [metric()],\n                                   units =  string(),\n                                   title =  string()}}\n               | [metric()]\n               | metric().\nmetric()      :: {Name :: string(), Type :: metric_type() }\n               | {Name :: string(), Type :: metric_type(), Opts :: map()}.\nmetric_type() :: counter | gauge | histogram.  This structure supports a three-level hierarchy:   Group of graphs is placed on the top of this hierarchy. It consists of the one or more graphs and define a group of graphs under the same name.  Graph consists of the one or more metrics that will be plotted on the same chart. Furthermore, you could specify additional option for the chart (e.g. units, title etc)  Metric is the lowest unit of this hierarchy. It specifies the name and type of the user-defined metric.   Let see the following metrics declaration:  metrics() -  [{group, \"HTTP Requests\", [\n                {graph, #{metrics =  [{\"success_requests\", counter}, {\"failed_requests\", counter}]}},\n                {graph, #{title =  \"Request's latency\",\n                          units =  \"ms\",\n                          metrics =  [{\"latency\", histogram}]}}]}].  In this example, a group of graphs with name  HTTP Requests  will be created. It will consist of the several graphs presented the number of success/failed requests and the request s latencies. Please note then a graph could produce several charts. In the mentioned example, the graph for success and failed request will produce two charts: absolute counters and their rps.  Derived metrics  Derived metrics are basically gauges which are evaluated on the director node every ~10sec. To define a derived metric  resolver  function should be specified in metric opts dictionary. Resolver function is used to evaluate metric value.\nTypical example of the derived metrics is the current number of pending requests. We specify a function (pending_requests) to calculate the metric value in metrics options and then define the function as simple difference between number of sent requests and number of reeived responses:  metrics() -  [{group, \"Requests\", [\n                {graph, #{metrics =  [\n                    {\"requests_sent\", counter},\n                    {\"responses_received\", counter},\n                    {\"pending_requests\", derived, #{resolver =  pending_requests}}]}},\n                ]}].\n\npending_requests() - \n    mzb_metrics:get_value(\"requests_sent\") - mzb_metrics:get_value(\"responses_received\").  Hooks  Pre and post hooks allow to run a custom code before or after benchmark. Hooks could be applied on every nodes or on the director node only. You are able to change any environment variable in your hook handler and use it in your scenario.  Scenario:  {pre_hook, [\n    {exec, all, \"yum install zlib\"},\n    {worker_call, fetch_commit, my_worker}\n]}\n\n{pool, [{size, 3}, {worker_type, dummy_worker}], [\n    {loop, [{time, {1, sec}},\n            {rate, {ramp, linear, {10, rps}, {50, rps}}}],\n        [{print, {var, \"commit\", \"default\"}}]}]},  Worker:  fetch_commit(Env) - \n    {ok, [{\"commit\", \"0123456\"} | Env]}.  Updating metrics  You can update a declared metric from anywhere inside your  worker . Simply call the following function:  mzb_metrics:notify({\" metric_name \",  metric_type },  value )  The tuple  {\" metric_name \",  metric_type }  is the same that was used during the metric declaration and identifies the metric to update. The  value  is the value to add to the metric.", 
            "title": "Metrics"
        }, 
        {
            "location": "/cloud_plugin/", 
            "text": "How to Write a Cloud Plugin\n\n\nCloud plugin is responsible for cluster allocation and deallocation, it is required to implement the following methods:\n\n\n-spec start(Name, Opts) -\n PluginRef when\n    Name :: atom(),\n    Opts :: #{},\n    PluginRef :: term().\n\n-spec create_cluster(PluginRef, NumNodes, Config) -\n {ok, ClusterID, UserName, [Host]} when\n    PluginRef :: term(),\n    NumNodes :: pos_integer(),\n    Config :: #{},\n    ClusterID :: term()\n    UserName :: string(),\n    Host :: string().\n\n-spec destroy_cluster(ClusterID) -\n ok when\n    ClusterID :: term().\n\n\n\nstart\n function starts the particular instance of the plugin and returns instance reference\n\n\n\n\nName - The name of the particular instance of the plugin (specfied in the configuration file).\n\n\nOpts - Options passed from the server configuration file for a particular instance of the plugin.\n\n\nNumNodes - Number of nodes to be allocated.\n\n\nConfig - Consists of user, name, description and exclusive_node_usage fields.\n\n\n\n\ncreate_cluster\n function should allocate required number of hosts and return a tuple of {ok, ClusterID, UserName, HostList}.\n\n\n\n\nClusterID - This term will be passed to destroy_cluster/1 when the benchmarking system wants to deallocate the compute nodes. Its content is up to the plugin developer.\n\n\nUserName - The benchmarking system will use this user name to ssh to the allocated compute nodes.\n\n\nHostList - is the list of hostnames or IPs of allocated nodes.\n\n\n\n\nUsing a cloud plugin\n\n\nOnce you have a plugin, it should be specified at MZBench config file inside mzbench_api section:\n\n\n[\n  {mzbench_api, [\n    {cloud_plugins, [{my_cloud1, #{application =\n privatecloud_plugin,\n                                   Opt1 =\n Value1,\n                                   Opt2 =\n Value2, ...}},\n                     ...\n                    ]},\n    {plugins_dir, \"../../plugins\"}\n    ]},\n].\n\n\n\nThis file is normally located at \n/etc/mzbench/server.config\n or \n~/.config/mzbench/server.config\n.\n\n\nPlugin-specific config could be specified at the same file inside corresponding section like at the example above. This config is optional.\n\n\nNote: Plugin application will be started using application:ensure_all_started/1 just before the benchmarks start\n\n\nPlugin binaries should be placed inside \nplugins_dir\n, for \n../../plugins/\n it would be \nmzbench/server/plugins/privatecloud-0.1.1/ebin/privatecloud.ebin", 
            "title": "Cloud Plugins"
        }, 
        {
            "location": "/cloud_plugin/#how-to-write-a-cloud-plugin", 
            "text": "Cloud plugin is responsible for cluster allocation and deallocation, it is required to implement the following methods:  -spec start(Name, Opts) -  PluginRef when\n    Name :: atom(),\n    Opts :: #{},\n    PluginRef :: term().\n\n-spec create_cluster(PluginRef, NumNodes, Config) -  {ok, ClusterID, UserName, [Host]} when\n    PluginRef :: term(),\n    NumNodes :: pos_integer(),\n    Config :: #{},\n    ClusterID :: term()\n    UserName :: string(),\n    Host :: string().\n\n-spec destroy_cluster(ClusterID) -  ok when\n    ClusterID :: term().  start  function starts the particular instance of the plugin and returns instance reference   Name - The name of the particular instance of the plugin (specfied in the configuration file).  Opts - Options passed from the server configuration file for a particular instance of the plugin.  NumNodes - Number of nodes to be allocated.  Config - Consists of user, name, description and exclusive_node_usage fields.   create_cluster  function should allocate required number of hosts and return a tuple of {ok, ClusterID, UserName, HostList}.   ClusterID - This term will be passed to destroy_cluster/1 when the benchmarking system wants to deallocate the compute nodes. Its content is up to the plugin developer.  UserName - The benchmarking system will use this user name to ssh to the allocated compute nodes.  HostList - is the list of hostnames or IPs of allocated nodes.", 
            "title": "How to Write a Cloud Plugin"
        }, 
        {
            "location": "/cloud_plugin/#using-a-cloud-plugin", 
            "text": "Once you have a plugin, it should be specified at MZBench config file inside mzbench_api section:  [\n  {mzbench_api, [\n    {cloud_plugins, [{my_cloud1, #{application =  privatecloud_plugin,\n                                   Opt1 =  Value1,\n                                   Opt2 =  Value2, ...}},\n                     ...\n                    ]},\n    {plugins_dir, \"../../plugins\"}\n    ]},\n].  This file is normally located at  /etc/mzbench/server.config  or  ~/.config/mzbench/server.config .  Plugin-specific config could be specified at the same file inside corresponding section like at the example above. This config is optional.  Note: Plugin application will be started using application:ensure_all_started/1 just before the benchmarks start  Plugin binaries should be placed inside  plugins_dir , for  ../../plugins/  it would be  mzbench/server/plugins/privatecloud-0.1.1/ebin/privatecloud.ebin", 
            "title": "Using a cloud plugin"
        }
    ]
}